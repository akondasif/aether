Aether disk search engine

0. Introduction
Aether is an application for indexing and searching unstructured collections.
Unstructured, in a way that the collections may be overlapping, may be randomly put together, or may be too large to go through manually.
The collections themselves is meant to be mainly books and documents, but also music may work.
For items like music we are solely dependent on indexing metadata.
Since the collections may be overlapping, the main id is an (md5) checksum, and it won't be indexed again if already found.
The application consists of a web client, that may connect to a server.
Both searching and indexing is initiated through the web client.
The server may be a single server, or a set of clustered/cloudy servers.
It is currently set for a standard analyzer (presumably for English), and is automatically able to recognize language.
Given a bit of manual training with classifier software, it is possible to use the training data to automatically classify the documents during indexing.

1. Installing

Use Java 8.
Unpack language profiles (see 2.1.3.3).
Install other software, like calibre, pdftotext, djvutxt, wv, tesseract.
Run (after configuring) with mvn package, mvn jetty:run

2. Configuration
Sample files are available.
For standalone single node: aether.prop.standalone
For standalone using cloud services: aether.prop.cloud
For two nodes: aether.prop.testnode1 aether.prop.testnode2

There are two modes, standalone and multi mode.
The standalone is set if hibernate or lucene is chosen.
(Assuming multinodes connect to the same Solr and Hbase, it is not checked.)
If standalone is used, the nodename will be localhost automatically,
otherwise a nodename will have to be set.
Standalone will use no global locker.

The content source is a directory list in dirlist.
The directory list may be ordinary, local filesystem, or it may be HDFS.
(More about HDFS later)
The exceptions are to be listedd in dirlistnot.

The indexed filenames and metadata are stored in a database.
This may either be the standalone H2 (with Hibernate) or the multinode Hbase (with or without DataNucleus).
The standalone is currently too slow to handle as big disks/sizes as multimode.

The indexed file content and metadata are stored in Lucene or Solr.
Lucene is the standalone, while Solr is the multinode.

If using multinode, the setting distributedlockmode is for setting
big or small granularity using Curator, if the mode is big only
one node may use the indexer or other write operations at one single time.

If using the small granularity, the locking will be on the level on the
indexed item.

If using multinode, and setting distributedprocess, it will use
Hazelcast to distribute through the nodes, and use Hazelcast locking.

Configure aether.prop:

2.1 Mandatory configuration.
2.1.1 General
Which directories to include, and subdirectories to exclude.
dirlist=file:/home/dir/dir1,hdfs:/home/dir/dir2...
dirlistnot=/home/dir/dir1/tmp (just pure /, no file:/hdfs:)
Known uses are file, hdfs and none (defaulting to file).

Name of the node.
Nodename=...
(But if db is hibernate or lucene, it is localhost anyway).

How many to max reindex each turn, or 0 for no limit.
reindexlimit=10000

How many to max index each turn, or 0 for no limit.
indexlimit=10000

The (re)indexing limits may be exceeded a bit, due to the parallell nature
of the application.

Don't try to index if the file has failed this many times
failedlimit=10

Number of seconds for Tika conversion timeout
tikatimeout=600

Number of seconds for Other conversion timeout
othertimeout=600

2.1.2 Database
The database may be HBase, DataNucleus/HBase or Hibernate/H2.

2.1.2.1 Hibernate/H2
Hibernate/H2 relevant settings:
db=hibernate
h2dir=...
Hibernate/H2 is for standalone only.

2.1.2.2 Hbase
Hbase relevant settings:
db=hbase
hbasequorum=localhost
hbaseport=2181
hbasemaster=localhost:2181

2.1.2.3 DataNucleus/HBase
DataNucleus/HBase elevant settings:
db=datanucleus

And before starting jetty, do this in a hbase shell:
create 'IndexFiles', { NAME => 'IndexFiles' }
create 'Files', { NAME => 'Files' }
(A temporary fix until datanucleus creates it itself)

2.1.3 Indexing/search:
Can be Lucene directly or Solr.
Lucene is for standalone only.

2.1.3.1 Lucene
Lucene relevant settings:
index=lucene
lucenepath=...

2.1.3.2 Solr
Solr relevant settings:
index=solr
solrurl=http://localhost:8983/solr/mystuff

mkdir -p server/solr/MYCORE/conf
rsync -v -a server/solr/configsets/basic_configs/conf/  server/solr/MYCORE/conf
if creating manually, create server/solr/MYCORE/core.properties
go to server/solr/MYCORE/conf
in server/solr/MYCORE/conf apply patch with
patch -p0 < core.nostore.patch
(or core.store.patch, if using highlight and morelikethis)

2.1.3.3 Langdetect
Download from https://code.google.com/p/language-detection/wiki/Downloads,
and put profiles* in the same dir as pom.xml.
Languages to be used for classification, configure (default en) with:
languages=en,fr

2.1.4 Distributed mode
There are two config settings for this.
With distributedlockmode=small there is a lock for each index entry.
With distributedprocess=true the files are placed in a distributed queue.

2.2 Non-mandatory

2.2.1 General
If using Hadoop HDFS
hdfsconffs=hadoop uri (sets fs.default.name)

Where to log
logdir=...

Whether to enable downloading
downloader=true

Whether to use authenication
authenticate=true
The admin user has username/password admin/admin, and the user has user/user.
The admin has access to the control panel and configuration, and will see more search results than the user.
The admin user will get results regarding indexing time usage and error messages from the indexing.
If no authentication is used, everything is open.
Accessing from localhost will determine admin right in both cases.

mltcount=...
mltmindf=...
mltmintf...
For setting More Like This search document count, minimum term frequency and minimum document frequency.
These settings may influence the time More Like Queries take.

2.2.2 Classifying
There are two types, Mahout and OpenNLP.

2.2.2.1 Mahout
Mahout relevant settings:

For the older map reduce based:
classify=mahout
mahoutbasepath=.../mahoutc/LANG
mahoutalgorithm=bayes (or cbayes)
mahaoutmodelpath=.../mahout/model
mahoutlabelindexfilepath=.../mahout/labelindex
mahoutdictionarypath=.../mahout/dataset-vectors/dictionary.file-0
mahoutdocumentfrequencypath=.../mahout/dataset-vectors/df-count/part-r-00000
mahaoutconffs=hadoop uri (sets fs.default.name in this specific case)

For the newer Spark baed:
classify=mahoutspark
mahoutbasepath=.../mahoutc/LANG
mahoutalgorithm=bayes (or cbayes)
mahaoutmodelpath=.../mahout/model
mahoutdictionarypath=.../mahout/dataset-vectors/dictionary.file-0
mahoutdocumentfrequencypath=.../mahout/dataset-vectors/df-count/part-r-00000
mahaoutconffs=hadoop uri (sets fs.default.name in this specific case)
mahoutsparkmaster=spark-master

The LANG will be replaced by the detected languages configured, so the files
and directories will be required to exist.
The mahoutbasepath, if existing, will just be prepended to the other paths,
which then will just indicate relative paths.

Training could be based on the Bayes part of 
examples/bin/classify-20newsgroups.sh in the Mahout distribution.

2.2.2.2 OpenNLP
OpenNLP relevant settings:
classify=opennlp
opennlpmodelpath=.../opennlp/LANG-doccat.bin
The LANG will be replaced by the detected languages configured, so the file
will be required to exist.

Training is standard, done with ./bin/opennlp DoccatTrainer in the
OpenNLP distribution.

2.2.3 Zookeeper
Configured by
zookeeper=...
Should be used in a multinode environment (not yet mandatory).

2.2.4 Highlight and MoreLikeThis
Configured by
highlightmlt=true

If using Solr, also

go to server/solr/MYCORE/conf
in server/solr/MYCORE/conf apply patch with
patch -p0 < core.store.patch

Beware that this also stores the content, and increases disk space usage.

2.2.5 Metadata

Tika is providing the metadata, but it is not always sufficient.
Therefore, have added a property Files-Content-Type for content type probed
by the Files class, in cases not supported by Tika.

3. GUI

3.1 Search
Use Lucene syntax
Searchable fields:
Id, lang, metadata, content (Lucene), content (Solr).

The metadata is stored by key/value pairs, and are searchable by metadata:key\:text (note the escaping of the :).

Query parser is behaving differently according to Lucene or Solr, see specific docs.

If searching in Solr with other search fields than default, they will set the query parser type, so you will not have to write {!queryparser} to use them.

https://cwiki.apache.org/confluence/display/solr/The+Standard+Query+Parser

https://cwiki.apache.org/confluence/display/solr/Other+Parsers

Search results.
The admin user gets more fields.
Note that when hovering over metadata, the metadata is shown.
Under "highlight and similar", by clicking the highlighted part, you will trigger a search for similar documents (More Like This).

3.2 Control panel
The control panel is admin functionality not seen by the user.
Here we see adding content from the file system, index, reindexing, consistency checks, cleaning, statistics, database querying.

Index filesystem new items
Button: Crawl the whole filesystem for new, and index.
Textfield: Use only the given path.
Filesystem index on changed md5 checks if md5 has changed, and reindexes.

Filesystem add new
Button: Add new files from the filesystem, but do not index.
Textfield: Use only the given path.

Index non-indexed items:
Button: Index all previously non-indexed (including failed).
Textfield: Use only the given path.
Reindex/index of suffix: Try to reindex/index all known items with suffix.

Reindex, textfield, use only the given path.
Reindex on before or after given date.

Reindex specific languages, found and configured.

Delete from database and lucene/solr (but does not actually delete from disk).

Consistency checks, with cleaning.
It lists files not currenly present in the filesystem (will be cleaned).
It also lists non-existing files (encoding problems) and new files.

Stats, listing non-indexed, memory usage or overlapping directories.

Database, get items related to md5 id.
Database search, fields indexed, convertsw, classification, failedreason, noindexreason, timeoutreason, language.

To get the configuration of other nodes.

3.3 The node configuration

Setting limits for how many times an indexing may fail before being ignored, limits for max indexing and reindexing, timeout for tika and other (alternative) conversion.

Settings for More Like This, max number of hits, minimum term frequency and minimum doc frequency.

3.4 General

Status label has rows for f(ile) queue t(ika) queue, o(ther) queue, i(ndex) queue, queued / running.
Also, a d for counting database objects which are finished or to be temporarily saved.

4. Caution
Multitasking issues, some locking may be insufficient, it is at best at a very coarse level.
For standalone the ordinary java locking is using.
For multinode zookeeper is needed, or some concurrent modification will happen.
With zookeeper, only node may index at a given time due to database access.

5. Changes

Since 0.4, with 0.5:
Solr and Lucene 5
Highlight and MoreLikeThis

opennlpmodelpath=/home/user/data/opennlp/LANG-doccat.bin (before only en)
new property mahoutbasepath=/home/user/data/mahoutc/LANG
if present the rest just added, otherwise, use with old full path.
mahoutlabelindexfilepath=/labelindex (before /home/user/data/mahoutc/labelindex)

Classifier multilanguage support
new property comma-separated languages

0.6:
Metadata change handling, stored, multi-valued.
No more limit check for using alternative converters, only do that if tika crashes.
Java 8.
Usage of mime type probing.
DataNucleus table changes.

0.7
New distributed mode, with usage of Curator and Hazelcast.
A lot of refactoring have been done.

0.8
Redoing configuration, more validity checking.
Multinode configuration.

0.9
Mahout spark.

6. Version numbers of external software

Solr 6.0.0
Hbase 1.2.1
Mahout 0.12.0
Spark 1.5.2
OpenNLP 1.5.3
Curator 2.9.1
Hazelcast 3.5.4

7. Database dumps

echo "scan 'files',{COLUMNS=>'fi'}" | ./bin/hbase shell > /tmp/hbf
echo "scan 'index',{COLUMNS=>'if'}" | ./bin/hbase shell > /tmp/hbi
